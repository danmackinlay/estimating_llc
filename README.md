# Local Learning Coefficient Sampler Benchmarks

This repo contains code to estimate _Local Learning Coefficients (LLCs)_ for small neural networks using _stochastic gradient Langevin dynamics (SGLD)_, _Hamiltonian Monte Carlo (HMC)_, and _Microcanonical Langevin Monte Carlo (MCLMC)_.
There is little novel theory here; the goal is to provide implementations using standard industrial tooling such as  [BlackJAX](https://github.com/blackjax-devs/blackjax/tree/1.2.5) (sampling) and [ArviZ](https://python.arviz.org/) (diagnostics).


## Representative Diagnostics

**Running LLC estimates** — per-chain and pooled running estimates of LLC = n·β·(E[Lₙ] − L₀). The shaded band shows final mean ± 2·SE (based on effective sample size).

![SGLD running LLC](assets/readme/sgld_llc_running.png)
![HMC running LLC](assets/readme/hmc_llc_running.png)
![MCLMC running LLC](assets/readme/mclmc_llc_running.png)

**Key diagnostic indicators:**
- **Rank plots:** Uniform ranks indicate good mixing; deviations signal multimodality or poor convergence
- **ESS evolution:** Should grow steadily and plateau; slow growth indicates high autocorrelation
- **Energy diagnostics:** HMC shows tight Hamiltonian distributions; MCLMC shows centered energy changes
- **Theta traces:** Should look like stationary noise; trends or drifts indicate non-convergence

<small>More diagnostic plots available via `llc analyze` (autocorrelation, theta traces, energy histograms, etc.)</small>

## Quickstart

```bash
uv sync
uv run python -m llc run --preset=quick
uv run python -m llc analyze runs/<run_id> --which all --plots running_llc,rank,ess_evolution,autocorr,energy,theta
```

Outputs saved to `runs/<run_id>/` containing `config.json`, `metrics.json`, `{sgld,hmc,mclmc}.nc`, and `analysis/` PNGs.

## What's in a Run

```text
runs/<run_id>/
├── config.json          # full configuration (all parameters)
├── metrics.json         # summary statistics (LLC mean/SE, ESS, WNV, timings)
├── L0.txt               # baseline loss at ERM
├── sgld.nc              # ArviZ traces for SGLD
├── hmc.nc               # traces + acceptance + energies for HMC
├── mclmc.nc             # traces + energy deltas for MCLMC
└── analysis/            # generated by `llc analyze`
    ├── *_running_llc.png
    ├── *_rank.png
    └── ...
```

## Common Tasks

| Task | Command |
|------|---------|
| Local quick run | `uv run python -m llc run --preset=quick` |
| Local sweep (8 workers) | `uv run python -m llc sweep --workers=8` |
| Modal run | `uv run python -m llc run --backend=modal` |
| SLURM run | `uv run python -m llc run --backend=submitit` |
| Analyze saved run | `uv run python -m llc analyze runs/<run_id>` |
| Plot sweep results | `uv run llc plot-sweep` |
| Refresh README images | `uv run llc promote-readme-images` |

For backend-specific setup and configuration, see [docs/backends.md](docs/backends.md).

## Efficiency Metrics

We automatically compute efficiency metrics for all samplers:
- **ESS/sec** — wall-clock efficiency
- **ESS/FDE** — gradient-normalized efficiency (data-size-agnostic)
- **WNV** — work-normalized variance for fair comparisons

Results saved to `metrics.json` per run and `llc_sweep_results.csv` for sweeps. Visualize with `uv run llc plot-sweep`.

## Installation

```bash
uv sync

# Optional backends
uv sync --extra modal      # Modal serverless support
uv sync --extra slurm      # SLURM/submitit support
uv sync --all-extras       # Both backends
```

## Features

- **Unified CLI** for end-to-end LLC estimation pipeline
- **Three samplers:** SGLD (with optional preconditioning), HMC, MCLMC
- **Configurable targets:** ReLU/tanh/GeLU MLPs, analytical quadratic (for testing)
- **Rich data generation:** Gaussian, anisotropic, mixture distributions with various noise models
- **ArviZ integration:** Full convergence diagnostics (ESS, R̂, autocorrelation, rank plots)
- **Work-normalized metrics:** Fair efficiency comparisons across parameter dimensions
- **Caching system:** Deterministic run IDs prevent duplicate computation

## Documentation

- [Backends (Modal/SLURM setup)](docs/backends.md)
- [Caching behavior](docs/caching.md)
- [Preconditioned SGLD options](docs/sgld-precond.md)
- [Target functions and data generators](docs/targets.md)
- [BlackJAX API notes](docs/blackjax.md)

## Motivation

In [Singular Learning Theory (SLT)](https://singularlearningtheory.com), the Local Learning Coefficient (LLC) quantifies the effective local dimensionality of a model around a trained optimum. This repo provides implementations using standard industrial tooling (BlackJAX for sampling, ArviZ for diagnostics) to benchmark LLC estimation methods on small but non-trivial neural networks.

## License

MIT.