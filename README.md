# Local Learning Coefficient Sampler Benchmarks

In [Singular Learning Theory (SLT)](https://singularlearningtheory.com), the Local Learning Coefficient (LLC) quantifies the effective local dimensionality of a model around a trained optimum.

This repo provides benchmark estimators of LLC on small but non-trivial neural networks, using standard industrial tooling: [BlackJAX](https://github.com/blackjax-devs/blackjax/tree/1.2.5) (sampling) and [ArviZ](https://python.arviz.org/) (diagnostics).

## Quick Start

1. **Run** an experiment → writes `runs/<run_id>/{sgld,hmc,mclmc}.nc`, `metrics.json`, `config.json`, `L0.txt`
2. **Analyze** saved runs → generates PNGs in `runs/<run_id>/analysis/` (default) with `llc analyze`
3. **Promote** (optional) → copy select PNGs to `assets/readme/` for the README gallery


![SGLD running LLC](assets/readme/sgld_llc_running.png)
![HMC running LLC](assets/readme/hmc_llc_running.png)
![MCLMC running LLC](assets/readme/mclmc_llc_running.png)


**What to look for in the plots:**

- **Running LLC:** Curves should stabilize and agree across chains; divergence suggests poor mixing
- **Rank plots:** Near-uniform → good; spikes → multimodality or non-convergence
- **ESS evolution:** Should grow and plateau; flat/slow growth → high autocorrelation
- **Energy:** HMC energy density should be regular/tight; MCLMC energy changes centered with reasonable spread (not produced for SGLD)

More plots available via `llc analyze` (rank, ESS evolution, autocorrelation, theta traces, energy histograms).

```bash
uv sync
uv run llc run --preset=quick
uv run llc analyze runs/<run_id> --which all --plots running_llc,rank,ess_evolution,autocorr,energy,theta
```

## What's in a Run

```text
runs/<run_id>/
├── config.json          # full configuration (all parameters)
├── metrics.json         # summary statistics (LLC mean/SE, ESS, WNV, timings)
├── L0.txt               # baseline loss at ERM
├── sgld.nc              # ArviZ traces for SGLD
├── hmc.nc               # traces + acceptance + energies for HMC
├── mclmc.nc             # traces + energy deltas for MCLMC
└── analysis/            # generated by `llc analyze`
    ├── *_running_llc.png
    ├── *_rank.png
    └── ...
```

**Sample `metrics.json`:**
```json
{
  "hmc_llc_mean": 367.1,
  "hmc_llc_se": 9.27,
  "hmc_ess": 28.0,
  "hmc_wnv_time": 0.042,
  "hmc_timing_sampling": 12.83,
  "mclmc_llc_mean": 131.7,
  "sgld_llc_mean": 145.7
}
```

## Common Tasks

| Task | Command |
|------|---------|
| Local quick run | `uv run llc run --preset=quick` |
| Local sweep (8 workers) | `uv run llc sweep --workers=8` |
| Modal run | `uv run llc run --backend=modal` |
| SLURM run | `uv run llc run --backend=submitit` |
| Analyze saved run | `uv run llc analyze runs/<run_id>` |
| Plot sweep results | `uv run llc plot-sweep` |
| Refresh README images | `uv run llc promote-readme-images` |

For backend-specific setup and configuration, see [docs/backends.md](docs/backends.md).

## Sweeps = Many Runs + One CSV

`uv run llc sweep` launches a grid of runs and writes `llc_sweep_results.csv` summarizing each run (with a `run_dir` back-pointer to analyze individual runs).

Plot medians via `uv run llc plot-sweep --size-col target_params` and filter via `--filters "activation=relu,x_dist=gauss_iso"`.

## Efficiency Metrics

We compute efficiency and work-normalized variance from the saved traces:

- **ESS/sec** — statistical efficiency per wall-clock second: `ESS / seconds`
- **ESS/FDE** — statistical efficiency per full-data-equivalent gradient: `ESS / FDE`, where `FDE = (# full-loss evals) + (# minibatch grads) × (b/n)`
- **WNV (time/FDE)** — variance × cost for fair comparisons: `WNV_time = (sd² / ESS) × seconds`, `WNV_FDE = (sd² / ESS) × FDE`

Here `sd` is the Monte Carlo standard deviation of LLC; ESS is ArviZ bulk ESS. Results saved to `metrics.json` per run and `llc_sweep_results.csv` for sweeps.

## Installation

```bash
uv sync

# Optional backends
uv sync --extra modal      # Modal serverless support
uv sync --extra slurm      # SLURM/submitit support
```

## Features

- **Unified CLI** for end-to-end LLC estimation pipeline
- **Three samplers:** SGLD (with optional preconditioning), HMC, MCLMC
- **Configurable targets:** ReLU/tanh/GeLU MLPs, analytical quadratic (for testing)
- **ArviZ integration:** Full convergence diagnostics (ESS, R̂, autocorrelation, rank plots)
- **Caching system:** Deterministic run IDs prevent duplicate computation

## Documentation

- [Backends (Modal/SLURM setup)](docs/backends.md)
- [Caching behavior](docs/caching.md)
- [Preconditioned SGLD options](docs/sgld-precond.md)
- [Target functions and data generators](docs/targets.md)
- [BlackJAX API notes](docs/blackjax.md)

## License

MIT.